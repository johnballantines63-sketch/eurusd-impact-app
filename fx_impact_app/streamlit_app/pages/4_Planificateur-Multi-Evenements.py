import sys
from pathlib import Path

# Ajouter le dossier src au PYTHONPATH
src_path = Path(__file__).parent.parent.parent / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

# T√©l√©charger la base de donn√©es si n√©cessaire (une seule fois)
try:
    from download_database import download_database
    download_database()
except Exception as e:
    pass  # D√©j√† t√©l√©charg√©e ou erreur g√©r√©e ailleurs


"""
Planificateur Multi-√âv√©nements
Pr√©dictions combin√©es pour √©v√©nements simultan√©s avec latence, TTR et retracement
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import duckdb
import re
import plotly.graph_objects as go
import plotly.express as px

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from config import get_db_path
from event_families import FAMILY_PATTERNS
from forecaster_mvp import ForecastEngine
from scoring_engine import ScoringEngine
from latency_analyzer import LatencyAnalyzer  # ‚úÖ AJOUT IMPORT

st.set_page_config(page_title="Planificateur Multi-√âv√©nements", page_icon="üìÖ", layout="wide")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MIGRATION DB AUTOMATIQUE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
try:
    import sys
    from pathlib import Path
    migrate_path = Path(__file__).parent.parent.parent.parent
    if str(migrate_path) not in sys.path:
        sys.path.insert(0, str(migrate_path))
    from migrate_db import migrate_database
    migrate_database()
except Exception as e:
    pass  # Ignore erreurs migration (DB peut √™tre read-only sur cloud)


st.title("üìÖ Planificateur Multi-√âv√©nements")
st.markdown("**Pr√©dictions combin√©es avec Impact, Latence, TTR, Retracement + Classification Empirique**")

# Session state pour caching
if 'events_loaded' not in st.session_state:
    st.session_state.events_loaded = False
if 'future_events' not in st.session_state:
    st.session_state.future_events = None
if 'selected_events' not in st.session_state:
    st.session_state.selected_events = set()
if 'family_stats_cache' not in st.session_state:
    st.session_state.family_stats_cache = {}
if 'backtest_cache' not in st.session_state:
    st.session_state.backtest_cache = {}


# Fonctions


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# NOUVELLES FONCTIONS OPTIMIS√âES v8.0
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@st.cache_data(ttl=3600)
def load_precomputed_stats_from_db():
    """Charge stats pr√©-calcul√©es depuis DB"""
    try:
        conn = duckdb.connect(get_db_path(), read_only=True)
        query = """
            SELECT DISTINCT family, latency_median, latency_p20, latency_p80,
                   ttr_median, ttr_p20, ttr_p80, mfe_p80, n_events_latency
            FROM event_families WHERE latency_median IS NOT NULL
        """
        results = conn.execute(query).fetchall()
        conn.close()
        stats_dict = {}
        for row in results:
            stats_dict[row[0]] = {
                'latency_median': row[1], 'latency_p20': row[2], 'latency_p80': row[3],
                'ttr_median': row[4], 'ttr_p20': row[5], 'ttr_p80': row[6],
                'mfe_p80': row[7] if row[7] else 10.0, 'n_events': row[8]
            }
        return stats_dict
    except:
        return {}

def predict_impact_fast(family, surprise, precomputed_stats, years_back=3):
    """Version ULTRA-RAPIDE"""
    # Normaliser le nom de famille (espaces ‚Üí underscores)
    family_normalized = family.replace(' ', '_')
    if family_normalized in precomputed_stats:
        stats = precomputed_stats[family_normalized]
        mfe = stats['mfe_p80']
        impact_factor = min(2.0, 1.0 + (surprise / 100)) if surprise > 0.5 else 1.0
        impact = mfe * impact_factor
        direction = 1 if surprise > 0 else -1
        return {
            'predicted_pips': impact, 'direction': direction,
            'latency_median': stats['latency_median'], 'latency_p20': stats['latency_p20'],
            'latency_p80': stats['latency_p80'], 'ttr_median': stats['ttr_median'],
            'ttr_p20': stats['ttr_p20'], 'ttr_p80': stats['ttr_p80'],
            'n_similar': stats['n_events'], 'mfe_p80': stats['mfe_p80'], 'source': 'precomputed_db'
        }
    else:
        result = predict_impact(family, surprise, years_back)
        if result:
            result['source'] = 'calculated'
        return result


def identify_family(event_key):
    for family_name, pattern in FAMILY_PATTERNS.items():
        clean_pattern = pattern.replace('(?i)', '')
        if re.search(clean_pattern, event_key, re.IGNORECASE):
            return family_name
    return None


def get_future_events(date_from, date_to, countries):
    conn = duckdb.connect(get_db_path())
    
    country_filter = "', '".join(countries)
    
    query = f"""
    SELECT 
        e.ts_utc, e.event_key, e.country, e.importance_n,
        e.actual, e.forecast, e.previous,
        ef.empirical_score, ef.empirical_impact, ef.impact_level,
        ef.avg_movement_pips, ef.avg_latency_min, ef.reaction_rate
    FROM events e
    LEFT JOIN event_families ef 
        ON e.event_key = ef.event_key AND e.country = ef.country
    WHERE e.ts_utc >= '{date_from.strftime('%Y-%m-%d %H:%M')}'
      AND e.ts_utc <= '{date_to.strftime('%Y-%m-%d %H:%M')}'
      AND e.country IN ('{country_filter}')
    ORDER BY e.ts_utc
    """
    
    df = conn.execute(query).fetchdf()
    conn.close()
    
    if len(df) > 0:
        df['family'] = df['event_key'].apply(identify_family)
        df = df[df['family'].notna()]
    
    return df


def predict_impact(family, surprise, years_back=3):
    """
    Pr√©dit impact avec latence et TTR bas√©s sur historique r√©el (avec cache)
    ‚úÖ CORRECTION: Utilise LatencyAnalyzer pour latences pr√©cises
    """
    # V√©rifier cache
    cache_key = f"{family}_{years_back}"
    if cache_key in st.session_state.family_stats_cache:
        stats = st.session_state.family_stats_cache[cache_key]
    else:
        pattern = FAMILY_PATTERNS.get(family, '')
        if not pattern:
            # Pas de warning si appel√© depuis pr√©-chargement
            if surprise != 0:
                st.warning(f"‚ö†Ô∏è Pattern non trouv√© pour famille: {family}")
            return None
        
        try:
            # === CORRECTION : Utiliser LatencyAnalyzer pour latences ===
            analyzer = LatencyAnalyzer(get_db_path())
            
            # Calculer stats de latence avec LatencyAnalyzer (PR√âCIS)
            # ‚úÖ CORRECTION: Bons param√®tres selon latency_analyzer.py
            latency_stats = analyzer.calculate_family_latency_stats(
                family_pattern=pattern,
                threshold_pips=5.0,
                min_events=5,
                lookback_days=years_back * 365  # ‚úÖ C'est lookback_days !
            )
            
            # ‚úÖ V√©rification robuste
            if not latency_stats or not isinstance(latency_stats, dict):
                analyzer.close()
                return None
            
            if latency_stats.get('events_analyzed', 0) == 0:
                analyzer.close()
                if surprise != 0:
                    st.warning(f"‚ö†Ô∏è Aucun √©v√©nement historique trouv√© pour {family}")
                return None
            
            # V√©rifier structure initial_reaction
            if 'initial_reaction' not in latency_stats or not latency_stats['initial_reaction']:
                analyzer.close()
                return None
            
            analyzer.close()
            
            # === Utiliser ForecastEngine uniquement pour MFE (impact) ===
            engine = ForecastEngine(get_db_path())
            
            mfe_stats = engine.calculate_family_stats(
                pattern,
                horizon_minutes=60,
                hist_years=years_back,
                countries=None
            )
            
            engine.close()
            
            # Combiner les deux sources
            stats = {
                'n_events': latency_stats['events_analyzed'],
                
                # LATENCE depuis LatencyAnalyzer (CORRECT ‚úÖ)
                'latency_median': latency_stats['initial_reaction']['median_minutes'],
                'latency_p20': latency_stats['initial_reaction'].get('p20_minutes', 
                    latency_stats['initial_reaction']['median_minutes'] * 0.5),
                'latency_p80': latency_stats['initial_reaction'].get('p80_minutes', 
                    latency_stats['initial_reaction']['median_minutes'] * 1.5),
                
                # TTR = Latence √ó 2 (formule empirique optimale ‚úÖ)
                'ttr_median': latency_stats['initial_reaction']['median_minutes'] * 2,
                'ttr_p20': latency_stats['initial_reaction']['median_minutes'] * 1.5,
                'ttr_p80': latency_stats['initial_reaction']['median_minutes'] * 3,
                
                # MFE (impact) depuis ForecastEngine
                'mfe_p80': mfe_stats.get('mfe_p80', 10)
            }
            
        except KeyError as e:
            # Erreur structure de donn√©es
            if 'analyzer' in locals():
                analyzer.close()
            if 'engine' in locals():
                engine.close()
            if surprise != 0:
                st.error(f"‚ùå Erreur structure donn√©es pour {family}: cl√© manquante '{e}'")
            return None
        except ImportError as e:
            if surprise != 0:
                st.error(f"‚ùå Erreur import LatencyAnalyzer: {e}")
                st.info("üí° V√©rifiez que latency_analyzer.py existe dans fx_impact_app/src/")
            return None
        except Exception as e:
            if 'analyzer' in locals():
                analyzer.close()
            if 'engine' in locals():
                engine.close()
            if surprise != 0:
                st.error(f"‚ùå Erreur predict_impact pour {family}: {e}")
            return None
        
        # Mettre en cache
        st.session_state.family_stats_cache[cache_key] = stats
    
    if stats['n_events'] == 0:
        return None
    
    # Impact bas√© sur MFE P80 historique
    base_impact = stats['mfe_p80']
    
    # Direction selon surprise
    direction = 1 if surprise > 0 else -1
    
    # Ajustement proportionnel √† la surprise
    surprise_factor = min(abs(surprise) / 50.0, 2.0)
    adjusted_impact = base_impact * (0.5 + 0.5 * surprise_factor)
    
    return {
        'predicted_pips': adjusted_impact,
        'direction': direction,
        'latency_median': stats['latency_median'],
        'latency_p20': stats['latency_p20'],
        'latency_p80': stats['latency_p80'],
        'ttr_median': stats['ttr_median'],
        'ttr_p20': stats['ttr_p20'],
        'ttr_p80': stats['ttr_p80'],
        'n_similar': stats['n_events'],
        'mfe_p80': stats['mfe_p80']
    }


def calculate_fibonacci_levels(impact_pips, direction):
    """Calcule les niveaux de retracement Fibonacci"""
    levels = {
        '0%': 0,
        '23.6%': impact_pips * 0.236,
        '38.2%': impact_pips * 0.382,
        '50%': impact_pips * 0.5,
        '61.8%': impact_pips * 0.618,
        '78.6%': impact_pips * 0.786,
        '100%': impact_pips
    }
    
    if direction < 0:
        levels = {k: -v for k, v in levels.items()}
    
    return levels


def create_timeline_chart(predictions, weighted_latency, min_ttr):
    """Cr√©e timeline visuelle interactive avec Plotly"""
    
    fig = go.Figure()
    
    # R√©f√©rence T0 = premier √©v√©nement
    first_event_time = min(pd.to_datetime(p['event']['ts_utc']) for p in predictions)
    
    colors = px.colors.qualitative.Set2
    
    for i, pred in enumerate(predictions):
        event_time = pd.to_datetime(pred['event']['ts_utc'])
        time_offset = (event_time - first_event_time).total_seconds() / 60  # minutes
        
        family_name = f"{pred['event']['family']} ({pred['event']['country']})"
        color = colors[i % len(colors)]
        
        # Point √©v√©nement
        fig.add_trace(go.Scatter(
            x=[time_offset],
            y=[i],
            mode='markers',
            name=family_name,
            marker=dict(size=15, color=color, symbol='diamond'),
            hovertemplate=f"<b>{family_name}</b><br>" +
                         f"T+{time_offset:.0f} min<br>" +
                         f"Impact: {pred['predicted_pips']:.1f} pips<br>" +
                         "<extra></extra>"
        ))
        
        # Fen√™tre de r√©action (latence)
        latency_start = time_offset
        latency_end = time_offset + pred['latency_median']
        
        fig.add_trace(go.Scatter(
            x=[latency_start, latency_end],
            y=[i, i],
            mode='lines',
            name=f"{family_name} - Latence",
            line=dict(color=color, width=3),
            showlegend=False,
            hovertemplate=f"Latence: {pred['latency_median']:.0f} min<extra></extra>"
        ))
        
        # Fen√™tre de persistance (TTR)
        ttr_end = time_offset + pred['ttr_median']
        
        fig.add_trace(go.Scatter(
            x=[latency_end, ttr_end],
            y=[i, i],
            mode='lines',
            name=f"{family_name} - TTR",
            line=dict(color=color, width=3, dash='dash'),
            showlegend=False,
            hovertemplate=f"TTR: {pred['ttr_median']:.0f} min<extra></extra>"
        ))
    
    # Ligne verticale r√©action attendue (moyenne pond√©r√©e)
    fig.add_vline(
        x=weighted_latency,
        line_dash="dot",
        line_color="green",
        annotation_text=f"R√©action attendue ({weighted_latency:.0f} min)",
        annotation_position="top"
    )
    
    # Ligne verticale sortie sugg√©r√©e (min TTR)
    fig.add_vline(
        x=min_ttr,
        line_dash="dot",
        line_color="red",
        annotation_text=f"Sortie sugg√©r√©e ({min_ttr:.0f} min)",
        annotation_position="top"
    )
    
    fig.update_layout(
        title="Timeline des √âv√©nements et Fen√™tres de Trading",
        xaxis_title="Temps (minutes depuis premier √©v√©nement)",
        yaxis_title="√âv√©nements",
        yaxis=dict(
            tickmode='array',
            tickvals=list(range(len(predictions))),
            ticktext=[f"{p['event']['family']}" for p in predictions]
        ),
        height=400,
        hovermode='closest',
        showlegend=True,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )
    
    return fig


def detect_overlaps(predictions):
    """D√©tecte les chevauchements entre fen√™tres d'√©v√©nements"""
    overlaps = []
    
    for i, pred1 in enumerate(predictions):
        time1 = pd.to_datetime(pred1['event']['ts_utc'])
        end1 = time1 + timedelta(minutes=pred1['ttr_median'])
        
        for j, pred2 in enumerate(predictions[i+1:], start=i+1):
            time2 = pd.to_datetime(pred2['event']['ts_utc'])
            start2 = time2
            
            # Chevauchement si √©v√©nement 2 d√©marre avant fin de TTR √©v√©nement 1
            if start2 < end1:
                overlap_minutes = (end1 - start2).total_seconds() / 60
                overlaps.append({
                    'event1': f"{pred1['event']['family']} ({pred1['event']['country']})",
                    'event2': f"{pred2['event']['family']} ({pred2['event']['country']})",
                    'overlap_minutes': overlap_minutes,
                    'severity': 'HIGH' if overlap_minutes > 10 else 'MEDIUM'
                })
    
    return overlaps


def calculate_tradability_score(predictions, overlaps, time_span):
    """Calcule un score de tradabilit√© de 0-100 pour la session"""
    score = 50  # Base
    
    # Bonus : nombre d'√©v√©nements
    if len(predictions) == 2:
        score += 10
    elif len(predictions) >= 3:
        score += 5
    
    # Bonus : coh√©rence directionnelle
    directions = [p['direction'] for p in predictions]
    if len(set(directions)) == 1:
        score += 20  # Amplification
    else:
        score -= 10  # Antagonisme
    
    # Bonus : impact total significatif
    total_impact = sum(abs(p['predicted_pips'] * p['direction']) for p in predictions)
    if total_impact > 20:
        score += 15
    elif total_impact > 10:
        score += 10
    
    # Malus : chevauchements
    high_overlaps = len([o for o in overlaps if o['severity'] == 'HIGH'])
    score -= high_overlaps * 10
    
    # Malus : √©v√©nements trop espac√©s
    if time_span > 3:
        score -= 15
    
    # Bonus : fen√™tre compacte
    if time_span < 1:
        score += 10
    
    return max(0, min(100, score))


def get_real_prices_batch(event_times, window_minutes=60):
    """R√©cup√®re les prix r√©els pour plusieurs √©v√©nements en UNE SEULE query (OPTIMIS√â)"""
    conn = duckdb.connect(get_db_path())
    
    results = {}
    
    # Convertir tous les timestamps
    epochs = []
    for i, event_time in enumerate(event_times):
        if isinstance(event_time, str):
            event_time = pd.to_datetime(event_time)
        
        if hasattr(event_time, 'tz') and event_time.tz is not None:
            event_time = event_time.tz_convert('UTC').tz_localize(None)
        elif hasattr(event_time, 'tz_localize'):
            event_time = pd.Timestamp(event_time).tz_localize('UTC').tz_localize(None)
        else:
            event_time = pd.Timestamp(event_time)
        
        event_epoch = int(event_time.timestamp())
        end_epoch = event_epoch + (window_minutes * 60)
        epochs.append((i, event_epoch, end_epoch))
    
    # UNE SEULE query pour tous les √©v√©nements
    if len(epochs) > 0:
        # Cr√©er conditions OR pour tous les √©v√©nements
        conditions = " OR ".join([f"(timestamp >= {e[1]} AND timestamp <= {e[2]})" for e in epochs])
        
        query = f"""
        SELECT timestamp, close
        FROM prices_1m
        WHERE {conditions}
        ORDER BY timestamp ASC
        """
        
        try:
            all_prices = conn.execute(query).fetchall()
            conn.close()
            
            # Dispatcher les prix vers chaque √©v√©nement
            for i, event_epoch, end_epoch in epochs:
                event_prices = [(t, p) for t, p in all_prices if event_epoch <= t <= end_epoch]
                
                if len(event_prices) > 0:
                    times = [datetime.fromtimestamp(r[0]) for r in event_prices]
                    prices = [r[1] for r in event_prices]
                    results[i] = pd.DataFrame({'time': times, 'price': prices})
                else:
                    results[i] = None
        except Exception as e:
            print(f"Erreur get_real_prices_batch: {e}")
            conn.close()
            return {}
    else:
        conn.close()
    
    return results


def measure_real_impact(prices_df, threshold_pips=5.0):
    """Mesure l'impact r√©el du march√© √† partir des prix"""
    if prices_df is None or len(prices_df) == 0:
        return None
    
    ref_price = prices_df.iloc[0]['price']
    
    # Trouver mouvement max et latence
    max_movement = 0
    latency_minutes = None
    peak_time = None
    direction = 0
    
    for i, row in prices_df.iterrows():
        movement_pips = (row['price'] - ref_price) * 10000
        
        if abs(movement_pips) > abs(max_movement):
            max_movement = movement_pips
            peak_time = i
        
        if latency_minutes is None and abs(movement_pips) >= threshold_pips:
            latency_minutes = i
            direction = 1 if movement_pips > 0 else -1
    
    # Trouver TTR (premier retournement significatif apr√®s le peak)
    ttr_minutes = None
    if peak_time is not None and peak_time < len(prices_df) - 1:
        peak_price = prices_df.iloc[peak_time]['price']
        
        for i in range(peak_time + 1, len(prices_df)):
            current_price = prices_df.iloc[i]['price']
            retracement = abs((current_price - peak_price) * 10000)
            
            # Retournement = retracement > 30% du mouvement initial
            if retracement > abs(max_movement) * 0.3:
                ttr_minutes = i - peak_time
                break
    
    if ttr_minutes is None:
        ttr_minutes = len(prices_df) - peak_time if peak_time else len(prices_df)
    
    return {
        'real_impact_pips': max_movement,
        'real_direction': direction,
        'real_latency_minutes': latency_minutes if latency_minutes is not None else len(prices_df),
        'real_ttr_minutes': ttr_minutes,
        'peak_time_minutes': peak_time,
        'had_reaction': latency_minutes is not None
    }


def create_backtest_chart(prices_df, event_time, predicted_impact, predicted_latency, predicted_ttr, real_metrics):
    """Cr√©e graphique comparaison pr√©diction vs r√©alit√©"""
    from datetime import timedelta
    
    fig = go.Figure()
    
    # Convertir event_time en datetime natif pour Plotly
    if isinstance(event_time, pd.Timestamp):
        event_time = event_time.to_pydatetime()
    
    # Convertir DataFrame times en datetime natifs
    plot_times = [t.to_pydatetime() if isinstance(t, pd.Timestamp) else t for t in prices_df['time']]
    
    # Prix r√©el
    fig.add_trace(go.Scatter(
        x=plot_times,
        y=prices_df['price'],
        mode='lines',
        name='Prix EUR/USD',
        line=dict(color='blue', width=2),
        hovertemplate='%{y:.5f}<extra></extra>'
    ))
    
    # Prix de r√©f√©rence (horizontal)
    ref_price = prices_df.iloc[0]['price']
    fig.add_hline(
        y=ref_price,
        line_dash="dash",
        line_color="gray",
        annotation_text="Prix r√©f√©rence"
    )
    
    # Ligne verticale √©v√©nement
    fig.add_shape(
        type="line",
        x0=event_time, x1=event_time,
        y0=0, y1=1,
        yref="paper",
        line=dict(color="black", width=2)
    )
    fig.add_annotation(
        x=event_time, y=1, yref="paper",
        text="üìä √âv√©nement",
        showarrow=False,
        yshift=10
    )
    
    # Ligne verticale latence pr√©dite
    predicted_latency_time = event_time + timedelta(minutes=float(predicted_latency))
    fig.add_shape(
        type="line",
        x0=predicted_latency_time, x1=predicted_latency_time,
        y0=0, y1=1,
        yref="paper",
        line=dict(color="orange", width=2, dash="dot")
    )
    fig.add_annotation(
        x=predicted_latency_time, y=0.9, yref="paper",
        text=f"Latence pr√©dite ({predicted_latency:.0f} min)",
        showarrow=False,
        font=dict(color="orange")
    )
    
    # Ligne verticale latence r√©elle
    if real_metrics and real_metrics['had_reaction']:
        real_latency_time = event_time + timedelta(minutes=float(real_metrics['real_latency_minutes']))
        fig.add_shape(
            type="line",
            x0=real_latency_time, x1=real_latency_time,
            y0=0, y1=1,
            yref="paper",
            line=dict(color="green", width=2, dash="dot")
        )
        fig.add_annotation(
            x=real_latency_time, y=0.1, yref="paper",
            text=f"Latence r√©elle ({real_metrics['real_latency_minutes']:.0f} min)",
            showarrow=False,
            font=dict(color="green")
        )
    
    # Ligne verticale TTR pr√©dit
    predicted_ttr_time = event_time + timedelta(minutes=float(predicted_ttr))
    fig.add_shape(
        type="line",
        x0=predicted_ttr_time, x1=predicted_ttr_time,
        y0=0, y1=1,
        yref="paper",
        line=dict(color="red", width=2, dash="dot")
    )
    fig.add_annotation(
        x=predicted_ttr_time, y=0.8, yref="paper",
        text=f"TTR pr√©dit ({predicted_ttr:.0f} min)",
        showarrow=False,
        font=dict(color="red")
    )
    
    # Zone impact pr√©dit
    predicted_price = ref_price + (predicted_impact / 10000)
    fig.add_hrect(
        y0=ref_price,
        y1=predicted_price,
        fillcolor="orange",
        opacity=0.2,
        line_width=0
    )
    
    fig.update_layout(
        title="Comparaison Pr√©diction vs R√©alit√© du March√©",
        xaxis_title="Temps",
        yaxis_title="Prix EUR/USD",
        height=500,
        hovermode='x unified',
        showlegend=True
    )
    
    return fig


# ‚úÖ PR√â-CHARGEMENT DES FAMILLES COMMUNES (Option 4)
# Plac√© ici car TOUTES les fonctions sont d√©finies
if 'preloaded' not in st.session_state:
    st.info("‚ö° Chargement stats DB...")
    precomputed_stats = load_precomputed_stats_from_db()
    if precomputed_stats:
        st.session_state.precomputed_stats = precomputed_stats
        st.session_state.preloaded = True
        st.success(f"‚úÖ {len(precomputed_stats)}/16 familles - Calculs ultra-rapides !", icon="‚ö°")
        with st.expander("üìä Familles disponibles"):
            for fam in sorted(precomputed_stats.keys()):
                st.caption(f"‚úÖ {fam}")
    else:
        st.warning("‚ö†Ô∏è Calculs classiques")
        st.session_state.precomputed_stats = {}
        st.session_state.preloaded = True
    

# === SIDEBAR ===
st.sidebar.header("‚öôÔ∏è Configuration")

# P√©riode
st.sidebar.subheader("üìÖ P√©riode")

mode_date = st.sidebar.radio(
    "Mode de s√©lection",
    ["Date pr√©cise", "P√©riode"],
    key='date_mode'
)

if mode_date == "Date pr√©cise":
    selected_date = st.sidebar.date_input(
        "Date",
        datetime.now().date() + timedelta(days=1),
        key='single_date'
    )
    date_from = datetime.combine(selected_date, datetime.min.time())
    date_to = datetime.combine(selected_date, datetime.max.time())
else:
    col1, col2 = st.sidebar.columns(2)
    with col1:
        date_from_input = st.date_input("De", datetime.now().date(), key='date_from')
        date_from = datetime.combine(date_from_input, datetime.min.time())
    with col2:
        date_to_input = st.date_input("√Ä", datetime.now().date() + timedelta(days=7), key='date_to')
        date_to = datetime.combine(date_to_input, datetime.max.time())

# Pays
countries = st.sidebar.multiselect(
    "Pays",
    ['US', 'EU', 'GB', 'JP', 'CH'],
    default=['US', 'EU'],
    key='countries_select'
)

# Charger √©v√©nements
if st.sidebar.button("üîç Charger √âv√©nements", type="primary", use_container_width=True):
    with st.spinner("Chargement..."):
        events = get_future_events(date_from, date_to, countries)
        
        if len(events) == 0:
            st.error("Aucun √©v√©nement trouv√©")
            st.stop()
        
        st.session_state.future_events = events
        st.session_state.events_loaded = True
        st.session_state.selected_events = set()

# === ZONE PRINCIPALE ===

if not st.session_state.events_loaded:
    st.info("üëà Configurez la p√©riode et cliquez sur Charger √âv√©nements")
    
    st.markdown("""
    ### üéØ Fonctionnement
    
    Cette page analyse **plusieurs √©v√©nements simultan√©s** avec :
    - **Score Empirique** : Classification bas√©e sur 3 ans de donn√©es r√©elles (0-100)
    - **Impact** : Mouvement prix pr√©dit (pips)
    - **Latence** : Temps avant r√©action du march√© (‚úÖ CORRIG√â avec LatencyAnalyzer)
    - **TTR** : Time To Reversal (persistance du mouvement)
    - **Retracement** : Niveaux Fibonacci de correction
    
    **M√©thode vectorielle** :
    ```
    Impact_combin√© = Œ£(impact_i √ó direction_i)
    Latence_combin√©e = moyenne pond√©r√©e
    TTR_combin√© = minimum (sortie au premier retournement)
    ```
    
    ### üìä Nouveaut√©s
    
    - üìà **Timeline visuelle** interactive
    - ‚ö†Ô∏è **D√©tection chevauchements** entre fen√™tres
    - üéØ **Score de tradabilit√©** 0-100
    - üìê **Niveaux Fibonacci** pour retracements
    - ‚úÖ **Latences pr√©cises** via LatencyAnalyzer
    
    ### üöÄ Workflow
    
    1. S√©lectionner p√©riode
    2. Charger √©v√©nements
    3. Cocher √©v√©nements √† analyser
    4. Entrer valeurs hypoth√©tiques
    5. Voir pr√©diction combin√©e + analyse compl√®te
    """)

else:
    df = st.session_state.future_events
    
    st.success(f"‚úÖ {len(df)} √©v√©nements trouv√©s")
    
    # Grouper par date
    df['date'] = pd.to_datetime(df['ts_utc']).dt.date
    dates = sorted(df['date'].unique())
    
    # S√©lection √©v√©nements
    st.header("üìã S√©lection des √âv√©nements")
    
    selected_indices = []
    
    for date in dates:
        st.subheader(f"üìÜ {date.strftime('%A %d/%m/%Y')}")
        
        day_events = df[df['date'] == date]
        
        for idx, event in day_events.iterrows():
            col1, col2, col3, col4, col5 = st.columns([0.5, 2, 1, 1, 1])
            
            with col1:
                checked = st.checkbox(
                    "",
                    value=idx in st.session_state.selected_events,
                    key=f"check_{idx}"
                )
                if checked:
                    selected_indices.append(idx)
            
            with col2:
                time_str = pd.to_datetime(event['ts_utc']).strftime('%H:%M')
                st.write(f"**{time_str}** - {event['family']} ({event['country']})")
                st.caption(event['event_key'])
            
            with col3:
                st.write(f"Previous: {event['previous'] if pd.notna(event['previous']) else 'N/A'}")
            
            with col4:
                st.write(f"Forecast: {event['forecast'] if pd.notna(event['forecast']) else 'N/A'}")
            
            with col5:
                # Score empirique
                if pd.notna(event.get('empirical_score')):
                    score = event['empirical_score']
                    impact_level = event.get('empirical_impact', 'N/A')
                    
                    # Couleur selon niveau
                    if score >= 70:
                        st.success(f"‚≠ê {score:.0f}")
                        st.caption(f"üî¥ {impact_level}")
                    elif score >= 40:
                        st.info(f"üìä {score:.0f}")
                        st.caption(f"üü° {impact_level}")
                    else:
                        st.warning(f"üìâ {score:.0f}")
                        st.caption(f"üü¢ {impact_level}")
                else:
                    st.caption("Score: N/A")
    
    st.session_state.selected_events = set(selected_indices)
    
    # Configuration des √©v√©nements s√©lectionn√©s
    if len(st.session_state.selected_events) > 0:
        st.divider()
        st.header("‚öôÔ∏è Configuration des √âv√©nements S√©lectionn√©s")
        
        predictions = []
        
        for idx in sorted(st.session_state.selected_events):
            event = df.loc[idx]
            
            with st.expander(f"üìä {event['family']} - {pd.to_datetime(event['ts_utc']).strftime('%H:%M')} ({event['country']})", expanded=True):
                
                # Afficher classification empirique en haut si disponible
                if pd.notna(event.get('empirical_score')):
                    col_class1, col_class2, col_class3 = st.columns(3)
                    with col_class1:
                        st.metric("üìä Score Empirique", f"{event['empirical_score']:.0f}/100")
                    with col_class2:
                        emp_impact = event.get('empirical_impact', 'N/A')
                        emoji = {"HIGH": "üî¥", "MEDIUM": "üü°", "LOW": "üü¢"}.get(emp_impact, "‚ö™")
                        st.metric("üéØ Impact Empirique", f"{emoji} {emp_impact}")
                    with col_class3:
                        theo_impact = event.get('impact_level', 'N/A')
                        emoji = {"HIGH": "üî¥", "MEDIUM": "üü°", "LOW": "üü¢"}.get(theo_impact, "‚ö™")
                        st.metric("üìñ Impact Th√©orique", f"{emoji} {theo_impact}")
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    previous = st.number_input(
                        "Previous",
                        value=float(event['previous']) if pd.notna(event['previous']) else 0.0,
                        step=0.1,
                        format="%.2f",
                        key=f"prev_{idx}"
                    )
                
                with col2:
                    reference = st.number_input(
                        "R√©f√©rence",
                        value=float(event['forecast']) if pd.notna(event['forecast']) else float(previous),
                        step=0.1,
                        format="%.2f",
                        key=f"ref_{idx}",
                        help="Forecast si dispo, sinon previous"
                    )
                
                with col3:
                    hypothetical = st.number_input(
                        "Actuel hypoth√©tique",
                        value=float(reference),
                        step=0.1,
                        format="%.2f",
                        key=f"hyp_{idx}"
                    )
                
                with col4:
                    surprise = hypothetical - reference
                    st.metric("Surprise", f"{surprise:+.2f}")
                
                # Pr√©diction individuelle
                if surprise != 0:
                    precomputed_stats = st.session_state.get('precomputed_stats', {})
                    pred = predict_impact_fast(event['family'], surprise, precomputed_stats)
                    
                    if pred:
                        predictions.append({
                            'event': event,
                            'surprise': surprise,
                            **pred
                        })
                        
                        direction_text = "üîº UP" if pred['direction'] > 0 else "üîΩ DOWN"
                        
                        # Affichage enrichi
                        col_a, col_b, col_c = st.columns(3)
                        
                        with col_a:
                            st.metric("Impact", f"{pred['predicted_pips']:.1f} pips", delta=direction_text)
                        
                        with col_b:
                            st.metric("Latence", f"{pred['latency_median']:.0f} min", 
                                     help=f"P20: {pred['latency_p20']:.0f} - P80: {pred['latency_p80']:.0f} min")
                        
                        with col_c:
                            st.metric("TTR", f"{pred['ttr_median']:.0f} min",
                                     help=f"P20: {pred['ttr_p20']:.0f} - P80: {pred['ttr_p80']:.0f} min")
                        
                        st.caption(f"Bas√© sur {pred['n_similar']} √©v√©nements historiques (MFE P80: {pred['mfe_p80']:.1f} pips)")
        
        # Pr√©diction combin√©e
        if len(predictions) > 1:
            st.divider()
            st.header("üé≤ Analyse Multi-√âv√©nements Compl√®te")
            
            # Analyser fen√™tre temporelle
            timestamps = [pd.to_datetime(p['event']['ts_utc']) for p in predictions]
            time_span = (max(timestamps) - min(timestamps)).total_seconds() / 3600
            
            # Calculs combin√©s
            vectorial_impact = sum(p['predicted_pips'] * p['direction'] for p in predictions)
            combined_direction = "üîº HAUSSE" if vectorial_impact > 0 else "üîΩ BAISSE"
            
            # Latence pond√©r√©e
            total_impact = sum(p['predicted_pips'] for p in predictions)
            if total_impact > 0:
                weighted_latency = sum(p['latency_median'] * p['predicted_pips'] for p in predictions) / total_impact
            else:
                weighted_latency = np.mean([p['latency_median'] for p in predictions])
            
            # TTR = minimum
            min_ttr = min(p['ttr_median'] for p in predictions)
            
            # D√©tection chevauchements
            overlaps = detect_overlaps(predictions)
            
            # Score de tradabilit√©
            tradability_score = calculate_tradability_score(predictions, overlaps, time_span)
            
            # === SECTION 1 : SCORE DE TRADABILIT√â ===
            st.subheader("üéØ Score de Tradabilit√© de la Session")
            
            score_col1, score_col2, score_col3 = st.columns([2, 1, 1])
            
            with score_col1:
                # Gauge visuelle
                if tradability_score >= 70:
                    score_color = "green"
                    score_label = "EXCELLENT"
                elif tradability_score >= 50:
                    score_color = "orange"
                    score_label = "BON"
                else:
                    score_color = "red"
                    score_label = "RISQU√â"
                
                st.metric("Score Global", f"{tradability_score}/100", delta=score_label)
            
            with score_col2:
                if time_span <= 2:
                    st.success(f"‚è±Ô∏è Fen√™tre compacte\n\n{time_span:.1f}h")
                else:
                    st.warning(f"‚ö†Ô∏è √âv√©nements espac√©s\n\n{time_span:.1f}h")
            
            with score_col3:
                directions = [p['direction'] for p in predictions]
                if len(set(directions)) == 1:
                    st.success("‚úÖ AMPLIFICATION\n\nM√™me direction")
                else:
                    st.warning("‚ö†Ô∏è ANTAGONISME\n\nDirections oppos√©es")
            
            # Alertes chevauchements
            if overlaps:
                st.warning(f"‚ö†Ô∏è **{len(overlaps)} chevauchement(s) d√©tect√©(s)**")
                for overlap in overlaps:
                    severity_emoji = "üî¥" if overlap['severity'] == 'HIGH' else "üü°"
                    st.caption(f"{severity_emoji} {overlap['event1']} ‚ÜîÔ∏è {overlap['event2']} : {overlap['overlap_minutes']:.0f} min de chevauchement")
            else:
                st.success("‚úÖ Aucun chevauchement d√©tect√©")
            
            st.divider()
            
            # === SECTION 2 : TIMELINE VISUELLE ===
            st.subheader("üìà Timeline Visuelle Interactive")
            
            timeline_fig = create_timeline_chart(predictions, weighted_latency, min_ttr)
            st.plotly_chart(timeline_fig, use_container_width=True)
            
            st.divider()
            
            # === SECTION 3 : D√âTAILS CALCUL ===
            st.subheader("üìä D√©tails du Calcul Vectoriel")
            
            calc_data = []
            for p in predictions:
                calc_data.append({
                    '√âv√©nement': f"{p['event']['family']} ({p['event']['country']})",
                    'Heure': pd.to_datetime(p['event']['ts_utc']).strftime('%H:%M'),
                    'Surprise': f"{p['surprise']:+.2f}",
                    'Impact': f"{p['predicted_pips']:.1f}",
                    'Direction': "üîº UP" if p['direction'] > 0 else "üîΩ DOWN",
                    'Latence': f"{p['latency_median']:.0f} min",
                    'TTR': f"{p['ttr_median']:.0f} min",
                    'Contribution': f"{p['predicted_pips'] * p['direction']:+.1f} pips"
                })
            
            st.table(pd.DataFrame(calc_data))
            
            st.divider()
            
            # === SECTION 4 : R√âSULTAT FINAL ===
            st.subheader("üéØ Impact Combin√© Final")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric(
                    "Impact Total",
                    f"{abs(vectorial_impact):.1f} pips",
                    delta=combined_direction
                )
            
            with col2:
                st.metric(
                    "Latence Attendue",
                    f"{weighted_latency:.0f} min",
                    help="Moyenne pond√©r√©e par impact"
                )
            
            with col3:
                st.metric(
                    "TTR Combin√©",
                    f"{min_ttr:.0f} min",
                    help="Premier retournement attendu"
                )
            
            with col4:
                cohesion = "Forte" if len(set([p['direction'] for p in predictions])) == 1 else "Faible"
                st.metric("Coh√©sion", cohesion)
            
            st.divider()
            
            # === SECTION 5 : RETRACEMENT FIBONACCI ===
            st.subheader("üìê Niveaux de Retracement Fibonacci")
            
            fib_levels = calculate_fibonacci_levels(abs(vectorial_impact), np.sign(vectorial_impact))
            
            fib_col1, fib_col2 = st.columns(2)
            
            with fib_col1:
                st.markdown("**Zones de Support/R√©sistance**")
                for level, pips in fib_levels.items():
                    if level in ['38.2%', '50%', '61.8%']:  # Niveaux cl√©s
                        st.info(f"**{level}** : {pips:+.1f} pips")
                    else:
                        st.caption(f"{level} : {pips:+.1f} pips")
            
            with fib_col2:
                st.markdown("**Recommandations**")
                st.write("üéØ **Zone d'entr√©e id√©ale** : 23.6% - 38.2%")
                st.write("‚ö†Ô∏è **Stop loss sugg√©r√©** : en dessous de 78.6%")
                st.write("üéÅ **Take profit** : 100% (mouvement complet)")
                st.write("üí∞ **TP partiel** : 61.8% (zone de r√©sistance)")
            
            st.divider()
            
            # === SECTION 6 : FEN√äTRE DE TRADING ===
            st.subheader("‚è∞ Fen√™tre de Trading Sugg√©r√©e")
            
            first_event_time = min(timestamps)
            
            entry_time = first_event_time - timedelta(minutes=2)
            reaction_time = first_event_time + timedelta(minutes=weighted_latency)
            exit_time = first_event_time + timedelta(minutes=min_ttr)
            
            col_t1, col_t2, col_t3 = st.columns(3)
            
            with col_t1:
                st.info(f"**üïê Entr√©e sugg√©r√©e**\n\n{entry_time.strftime('%H:%M')}\n\n(2 min avant)")
            
            with col_t2:
                st.success(f"**üìä R√©action attendue**\n\n{reaction_time.strftime('%H:%M')}\n\n(+{weighted_latency:.0f} min)")
            
            with col_t3:
                st.warning(f"**üéØ Sortie sugg√©r√©e**\n\n{exit_time.strftime('%H:%M')}\n\n(TTR √† {min_ttr:.0f} min)")
            
            # Plan de trading d√©taill√©
            with st.expander("üìã Plan de Trading D√©taill√©", expanded=False):
                st.markdown(f"""
                ### Phase 1 : Pr√©paration ({entry_time.strftime('%H:%M')})
                - ‚úÖ V√©rifier conditions de march√© (spread, liquidit√©)
                - ‚úÖ Placer ordres limite aux niveaux Fibonacci 23.6% - 38.2%
                - ‚úÖ D√©finir stop loss √† 78.6% = {fib_levels['78.6%']:+.1f} pips
                
                ### Phase 2 : Entr√©e (T0 = {first_event_time.strftime('%H:%M')})
                - üìä √âv√©nements : {', '.join([p['event']['family'] for p in predictions])}
                - üéØ Impact attendu : {abs(vectorial_impact):.1f} pips {combined_direction}
                - ‚è±Ô∏è R√©action dans {weighted_latency:.0f} minutes
                
                ### Phase 3 : Gestion ({reaction_time.strftime('%H:%M')} - {exit_time.strftime('%H:%M')})
                - üí∞ TP partiel √† 61.8% = {fib_levels['61.8%']:+.1f} pips
                - üéÅ TP final √† 100% = {fib_levels['100%']:+.1f} pips
                - ‚ö†Ô∏è Sortie compl√®te √† {exit_time.strftime('%H:%M')} (TTR)
                
                ### Phase 4 : Retracement √©ventuel
                - üìâ Si retracement, surveiller support 50% = {fib_levels['50%']:+.1f} pips
                - üîÑ Possible re-entr√©e si rebond confirm√© sur 50% ou 61.8%
                """)
            
            st.divider()
            
            # === SECTION 7 : SC√âNARIOS ===
            st.subheader("üé≠ Sc√©narios Alternatifs")
            
            scenarios = []
            for delta in [-2, -1, 0, 1, 2]:
                scenario_predictions = []
                
                for p in predictions:
                    new_surprise = p['surprise'] + delta
                    new_pred = predict_impact(p['event']['family'], new_surprise)
                    
                    if new_pred:
                        scenario_predictions.append({
                            'impact': new_pred['predicted_pips'] * new_pred['direction'],
                            'latency': new_pred['latency_median'],
                            'ttr': new_pred['ttr_median']
                        })
                
                if scenario_predictions:
                    scenario_impact = sum(sp['impact'] for sp in scenario_predictions)
                    scenario_latency = np.mean([sp['latency'] for sp in scenario_predictions])
                    scenario_ttr = min(sp['ttr'] for sp in scenario_predictions)
                    
                    scenarios.append({
                        'Variation': f"{delta:+d}",
                        'Impact': f"{abs(scenario_impact):.1f} pips",
                        'Direction': "üîº UP" if scenario_impact > 0 else "üîΩ DOWN",
                        'Latence': f"{scenario_latency:.0f} min",
                        'TTR': f"{scenario_ttr:.0f} min"
                    })
            
            if scenarios:
                st.table(pd.DataFrame(scenarios))
            
            st.divider()
            
            # === SECTION 8 : BACKTESTING (SI √âV√âNEMENTS PASS√âS) ===
            
            # V√©rifier si √©v√©nements sont dans le pass√©
            now = pd.Timestamp.now(tz='UTC')
            
            def to_utc_aware(ts):
                """Convertit timestamp en UTC aware"""
                ts = pd.to_datetime(ts)
                if ts.tz is None:
                    return ts.tz_localize('UTC')
                else:
                    return ts.tz_convert('UTC')
            
            is_past = all(to_utc_aware(p['event']['ts_utc']) < now for p in predictions)
            
            if is_past:
                st.header("üî¨ Backtesting - Comparaison Pr√©diction vs R√©alit√©")
                
                st.info("üìä Les √©v√©nements s√©lectionn√©s sont dans le pass√©. Analyse des r√©sultats r√©els...")
                
                # OPTIMISATION : R√©cup√©rer TOUS les prix en UNE SEULE query
                event_times = [pd.to_datetime(p['event']['ts_utc']) for p in predictions]
                
                with st.spinner("üì• R√©cup√©ration des prix r√©els (batch optimis√©)..."):
                    prices_batch = get_real_prices_batch(event_times, window_minutes=60)
                
                # Pour chaque √©v√©nement, mesurer impact r√©el
                backtest_results = []
                
                progress_bar = st.progress(0)
                for i, pred in enumerate(predictions):
                    event_time = pd.to_datetime(pred['event']['ts_utc'])
                    
                    # R√©cup√©rer depuis batch
                    prices_df = prices_batch.get(i)
                    
                    if prices_df is not None and len(prices_df) > 0:
                        # Mesurer impact r√©el
                        real_metrics = measure_real_impact(prices_df)
                        
                        if real_metrics:
                            backtest_results.append({
                                'prediction': pred,
                                'prices': prices_df,
                                'real_metrics': real_metrics,
                                'event_time': event_time
                            })
                    
                    progress_bar.progress((i + 1) / len(predictions))
                
                progress_bar.empty()
                
                if backtest_results:
                    # Tableau comparatif
                    st.subheader("üìä Tableau Comparatif Pr√©diction vs R√©alit√©")
                    
                    comparison_data = []
                    for result in backtest_results:
                        pred = result['prediction']
                        real = result['real_metrics']
                        
                        # Calcul erreurs
                        error_impact = abs(pred['predicted_pips'] - abs(real['real_impact_pips']))
                        error_latency = abs(pred['latency_median'] - real['real_latency_minutes'])
                        error_ttr = abs(pred['ttr_median'] - real['real_ttr_minutes'])
                        
                        comparison_data.append({
                            '√âv√©nement': f"{pred['event']['family']} ({pred['event']['country']})",
                            'Impact Pr√©dit': f"{pred['predicted_pips']:.1f} pips",
                            'Impact R√©el': f"{abs(real['real_impact_pips']):.1f} pips",
                            'Erreur Impact': f"{error_impact:.1f} pips",
                            'Latence Pr√©dite': f"{pred['latency_median']:.0f} min",
                            'Latence R√©elle': f"{real['real_latency_minutes']:.0f} min",
                            'Erreur Latence': f"{error_latency:.0f} min",
                            'TTR Pr√©dit': f"{pred['ttr_median']:.0f} min",
                            'TTR R√©el': f"{real['real_ttr_minutes']:.0f} min",
                            'Erreur TTR': f"{error_ttr:.0f} min"
                        })
                    
                    st.dataframe(pd.DataFrame(comparison_data), use_container_width=True)
                    
                    # M√©triques globales d'erreur
                    st.subheader("üéØ M√©triques d'Erreur Globales")
                    
                    errors_impact = [abs(r['prediction']['predicted_pips'] - abs(r['real_metrics']['real_impact_pips'])) 
                                    for r in backtest_results]
                    errors_latency = [abs(r['prediction']['latency_median'] - r['real_metrics']['real_latency_minutes']) 
                                     for r in backtest_results]
                    errors_ttr = [abs(r['prediction']['ttr_median'] - r['real_metrics']['real_ttr_minutes']) 
                                 for r in backtest_results]
                    
                    col_err1, col_err2, col_err3 = st.columns(3)
                    
                    with col_err1:
                        mae_impact = np.mean(errors_impact)
                        rmse_impact = np.sqrt(np.mean([e**2 for e in errors_impact]))
                        st.metric("MAE Impact", f"{mae_impact:.1f} pips")
                        st.caption(f"RMSE: {rmse_impact:.1f} pips")
                    
                    with col_err2:
                        mae_latency = np.mean(errors_latency)
                        rmse_latency = np.sqrt(np.mean([e**2 for e in errors_latency]))
                        st.metric("MAE Latence", f"{mae_latency:.1f} min")
                        st.caption(f"RMSE: {rmse_latency:.1f} min")
                    
                    with col_err3:
                        mae_ttr = np.mean(errors_ttr)
                        rmse_ttr = np.sqrt(np.mean([e**2 for e in errors_ttr]))
                        st.metric("MAE TTR", f"{mae_ttr:.1f} min")
                        st.caption(f"RMSE: {rmse_ttr:.1f} min")
                    
                    # Interpr√©tation
                    if mae_impact < 5:
                        st.success("‚úÖ Excellente pr√©cision sur l'impact (MAE < 5 pips)")
                    elif mae_impact < 10:
                        st.info("‚ÑπÔ∏è Bonne pr√©cision sur l'impact (MAE < 10 pips)")
                    else:
                        st.warning("‚ö†Ô∏è Pr√©cision mod√©r√©e sur l'impact (MAE ‚â• 10 pips)")
                    
                    if mae_latency < 5:
                        st.success("‚úÖ Excellente pr√©cision sur la latence (MAE < 5 min)")
                    elif mae_latency < 10:
                        st.info("‚ÑπÔ∏è Bonne pr√©cision sur la latence (MAE < 10 min)")
                    else:
                        st.warning("‚ö†Ô∏è Pr√©cision mod√©r√©e sur la latence (MAE ‚â• 10 min)")
                    
                    st.divider()
                    
                    # Graphiques individuels (lazy loading - seulement si demand√©)
                    st.subheader("üìà Graphiques Prix R√©els vs Pr√©dictions")
                    
                    show_charts = st.checkbox("üìä Afficher les graphiques d√©taill√©s (peut ralentir)", value=False)
                    
                    if show_charts:
                        for result in backtest_results:
                            pred = result['prediction']
                            real = result['real_metrics']
                            
                            with st.expander(f"üìä {pred['event']['family']} - {result['event_time'].strftime('%d/%m/%Y %H:%M')}", expanded=False):
                                
                                # Graphique
                                chart = create_backtest_chart(
                                    result['prices'],
                                    result['event_time'],
                                    pred['predicted_pips'] * pred['direction'],
                                    pred['latency_median'],
                                    pred['ttr_median'],
                                    real
                                )
                                
                                st.plotly_chart(chart, use_container_width=True)
                                
                                # R√©sum√©
                                col_sum1, col_sum2, col_sum3 = st.columns(3)
                                
                                with col_sum1:
                                    error_impact = abs(pred['predicted_pips'] - abs(real['real_impact_pips']))
                                    st.metric(
                                        "Impact",
                                        f"{abs(real['real_impact_pips']):.1f} pips (r√©el)",
                                        delta=f"{error_impact:.1f} pips erreur"
                                    )
                                
                                with col_sum2:
                                    error_latency = abs(pred['latency_median'] - real['real_latency_minutes'])
                                    st.metric(
                                        "Latence",
                                        f"{real['real_latency_minutes']:.0f} min (r√©el)",
                                        delta=f"{error_latency:.0f} min erreur"
                                    )
                                
                                with col_sum3:
                                    error_ttr = abs(pred['ttr_median'] - real['real_ttr_minutes'])
                                    st.metric(
                                        "TTR",
                                        f"{real['real_ttr_minutes']:.0f} min (r√©el)",
                                        delta=f"{error_ttr:.0f} min erreur"
                                    )
                                
                                # Direction
                                pred_dir = "üîº UP" if pred['direction'] > 0 else "üîΩ DOWN"
                                real_dir = "üîº UP" if real['real_direction'] > 0 else "üîΩ DOWN"
                                
                                if pred['direction'] == real['real_direction']:
                                    st.success(f"‚úÖ Direction correctement pr√©dite : {pred_dir} = {real_dir}")
                                else:
                                    st.error(f"‚ùå Direction incorrecte : pr√©dit {pred_dir}, r√©el {real_dir}")
                    else:
                        st.info("üí° Cochez la case ci-dessus pour afficher les graphiques individuels")
                
                else:
                    st.warning("‚ö†Ô∏è Impossible de r√©cup√©rer les prix r√©els pour ces √©v√©nements")
            
            st.divider()
            
            # === SECTION 9 : EXPORT ===
            st.subheader("üíæ Export de l'Analyse")
            
            export_data = {
                'date': date_from.strftime('%Y-%m-%d'),
                'n_events': len(predictions),
                'tradability_score': tradability_score,
                'combined_impact_pips': abs(vectorial_impact),
                'direction': 'UP' if vectorial_impact > 0 else 'DOWN',
                'latency_minutes': round(weighted_latency, 1),
                'ttr_minutes': round(min_ttr, 1),
                'entry_time': entry_time.strftime('%H:%M'),
                'reaction_time': reaction_time.strftime('%H:%M'),
                'exit_time': exit_time.strftime('%H:%M'),
                'fibonacci_levels': {k: round(v, 1) for k, v in fib_levels.items()},
                'overlaps': overlaps,
                'events': [
                    {
                        'family': p['event']['family'],
                        'country': p['event']['country'],
                        'time': pd.to_datetime(p['event']['ts_utc']).strftime('%H:%M'),
                        'surprise': round(p['surprise'], 2),
                        'predicted_pips': round(p['predicted_pips'], 1),
                        'direction': 'UP' if p['direction'] > 0 else 'DOWN',
                        'latency': round(p['latency_median'], 0),
                        'ttr': round(p['ttr_median'], 0)
                    }
                    for p in predictions
                ]
            }
            
            import json
            json_export = json.dumps(export_data, indent=2)
            
            st.download_button(
                "üì• T√©l√©charger Analyse Compl√®te (JSON)",
                json_export,
                f"multi_events_analysis_{date_from.strftime('%Y%m%d')}.json",
                "application/json",
                use_container_width=True
            )
        
        elif len(predictions) == 1:
            st.info("‚ÑπÔ∏è S√©lectionnez au moins 2 √©v√©nements pour voir l'analyse multi-√©v√©nements compl√®te")
